{
  "metadata_schema": {
    "description": "RAG chunks for Chapter 3: Sensors and Actuators from robotics curriculum",
    "version": "1.0",
    "fields": {
      "chunk_id": "string: Unique identifier following naming convention ch3-[section]-[chunk]",
      "chapter": "string: Chapter title",
      "chapter_number": "integer: 3",
      "section": "string: Section title or null for chapter-level content",
      "section_number": "integer: Section number (1-6)",
      "subsection": "string: Subsection title or null",
      "heading_level": "integer: Markdown heading level (1-4)",
      "parent_heading": "string: Immediate parent heading text",
      "chunk_order": "integer: Sequential order within chapter",
      "content": "string: The actual text content of the chunk",
      "content_type": "string: Type - paragraph|code|list|table|mixed",
      "difficulty": "string: beginner|intermediate|advanced",
      "keywords": "array: List of relevant keywords for retrieval",
      "char_count": "integer: Character count of content",
      "token_estimate": "integer: Estimated token count (approx 4 chars/token)"
    }
  },
  "processing_summary": {
    "source_file": "content.md",
    "total_chunks": 15,
    "total_tokens_estimated": 9850,
    "chunk_size_stats": {
      "min_tokens": 380,
      "max_tokens": 1020,
      "avg_tokens": 657
    },
    "keywords_covered": [
      "sensors", "actuators", "LiDAR", "RGB-D cameras", "IMU",
      "point cloud processing", "sensor fusion", "Kalman filter",
      "complementary filter", "Madgwick filter", "DC motor",
      "force-torque sensor", "camera calibration", "time-of-flight",
      "proprioception", "exteroception", "resolution", "accuracy",
      "precision", "latency", "Unitree G1", "RealSense D435i",
      "BMI088", "encoder", "actuator", "torque-speed curve"
    ]
  },
  "chunks": [
    {
      "chunk_id": "ch3-sec1-intro",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Sensor Taxonomy",
      "section_number": 1,
      "subsection": "The Foundation of Robotic Perception",
      "subsection_number": 1,
      "heading_level": 3,
      "parent_heading": "3.1 Sensor Taxonomy",
      "chunk_order": 1,
      "content": "A robot's ability to interact intelligently with its environment depends fundamentally on its capacity to perceive both its internal state and the world around it. Sensors are the interface between the digital brain of a robot and the physical reality it inhabits. Without sensors, a robot would be blind to its surroundings and oblivious to its own configuration—incapable of the feedback necessary for meaningful action. Understanding how to classify, characterize, and select sensors is therefore one of the most practical and essential skills in robotics engineering.",
      "content_type": "paragraph",
      "difficulty": "beginner",
      "keywords": ["robotic perception", "sensors overview", "sensor interface", "robot sensing"],
      "char_count": 489,
      "token_estimate": 122
    },
    {
      "chunk_id": "ch3-sec1-proprio-extero",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Sensor Taxonomy",
      "section_number": 1,
      "subsection": "Proprioception versus Exteroception",
      "subsection_number": 2,
      "heading_level": 3,
      "parent_heading": "3.1 Sensor Taxonomy",
      "chunk_order": 2,
      "content": "Sensors fall into two broad categories based on what they measure. Proprioceptive sensors report on the robot's own internal state: the angles of its joints, the speed of its wheels, the orientation of its body. These measurements stay within the robot, telling the control system where each part is and how it is moving. Joint encoders on the Unitree G1's 23 degrees of freedom exemplify proprioception—they track the angular position of each motor with 0.01 degree resolution, providing the feedback necessary for precise limb control.\n\nExteroceptive sensors, by contrast, reach outward to measure properties of the environment. LiDAR scanners map the geometry of a room, cameras capture visual scenes, microphones listen for sounds. These sensors provide the contextual information that allows a robot to navigate, recognize objects, and interact with people and things that are not part of its own body. The Unitree G1 combines both types: its RealSense D435i RGB-D camera is exteroceptive, sensing the world in front of it, while its BMI088 IMU is proprioceptive, reporting the robot's own acceleration and rotation.",
      "content_type": "paragraph",
      "difficulty": "beginner",
      "keywords": ["proprioceptive sensors", "exteroceptive sensors", "joint encoders", "Unitree G1", "IMU", "RGB-D camera"],
      "char_count": 681,
      "token_estimate": 170
    },
    {
      "chunk_id": "ch3-sec1-active-passive",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Sensor Taxonomy",
      "section_number": 1,
      "subsection": "Active versus Passive Sensors",
      "subsection_number": 3,
      "heading_level": 3,
      "parent_heading": "3.1 Sensor Taxonomy",
      "chunk_order": 3,
      "content": "Another dimension of classification concerns whether a sensor emits energy into the environment or merely receives what is already present. Passive sensors are observers: they wait for energy to arrive and then measure it. A standard camera is passive—it does not illuminate the scene but merely captures whatever light happens to be there. This makes passive sensors energy-efficient and unobtrusive, but they depend on adequate ambient conditions.\n\nActive sensors take a more assertive approach: they emit their own energy and measure the response. LiDAR scanners fire laser pulses and measure the returning light. Structured-light projectors cast known patterns onto surfaces and observe the deformation. Ultrasonic sensors emit sound waves and listen for echoes. Active sensors can operate in complete darkness and often provide more precise distance measurements, but they consume more power and can interfere with other robots using similar sensing modalities. The Unitree G1's Mid-360 LiDAR is active, continuously scanning its environment with rotating laser beams even in pitch-black conditions.",
      "content_type": "paragraph",
      "difficulty": "beginner",
      "keywords": ["active sensors", "passive sensors", "LiDAR", "structured light", "ultrasonic sensors", "energy emission"],
      "char_count": 621,
      "token_estimate": 155
    },
    {
      "chunk_id": "ch3-sec1-metrics",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Sensor Taxonomy",
      "section_number": 1,
      "subsection": "Performance Metrics for Sensor Characterization",
      "subsection_number": 4,
      "heading_level": 3,
      "parent_heading": "3.1 Sensor Taxonomy",
      "chunk_order": 4,
      "content": "When selecting and comparing sensors, engineers evaluate several key performance characteristics. Resolution describes the smallest change in the measured quantity that the sensor can detect. A joint encoder with 0.01 degree resolution can distinguish positions differing by one-hundredth of a degree. Higher resolution provides more detailed measurements but generates more data that must be processed and stored.\n\nAccuracy measures how close a sensor's readings are to the true value. A thermometer reading 20.1°C when the actual temperature is 20.0°C has an accuracy of 0.1°C. Accuracy is limited by systematic errors like calibration drift and is distinct from precision.\n\nPrecision (or repeatability) describes how consistently the sensor produces the same reading for the same true value. A precise sensor has low random variation, even if it is inaccurate due to calibration error. A sensor can be precise without being accurate, and accurate without being precise—the ideal sensor is both.\n\nRange defines the minimum and maximum values the sensor can measure. The RealSense D435i on the Unitree G1 operates from 0.3 to 10 meters. Field of view specifies the angular extent of the sensor's coverage—the Mid-360 LiDAR provides 360 degrees horizontally and 59 degrees vertically. Latency is the delay between a physical change and the sensor's report of that change. For real-time control systems, latency must be significantly shorter than the control loop period.",
      "content_type": "paragraph",
      "difficulty": "intermediate",
      "keywords": ["sensor resolution", "sensor accuracy", "sensor precision", "dynamic range", "field of view", "latency", "sensor specifications"],
      "char_count": 882,
      "token_estimate": 221
    },
    {
      "chunk_id": "ch3-sec2-lidar-principles",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "LiDAR Sensors",
      "section_number": 2,
      "subsection": "Ranging Through Time of Flight",
      "subsection_number": 1,
      "heading_level": 3,
      "parent_heading": "3.2 LiDAR Sensors",
      "chunk_order": 5,
      "content": "Light Detection and Ranging (LiDAR) sensors measure distance by timing the journey of light. A laser pulse travels at the speed of light—approximately 299,792,458 meters per second—and the sensor measures the elapsed time until a fraction of that light returns after reflecting from a surface. The distance d is computed from the round-trip time Delta t as: d = (c * Delta t) / 2, where c is the speed of light. This time-of-flight (ToF) principle enables precise distance measurements without physical contact with the target.\n\nTwo primary implementations of ToF exist in commercial LiDAR. Pulsed LiDAR emits short laser pulses (typically nanoseconds in duration) and precisely measures the arrival time of returning photons. This approach supports long ranges (100+ meters) and high power, making it common in autonomous vehicles. Phase-shift LiDAR emits continuous laser beams at known frequencies and measures the phase shift between emitted and returned signals. While phase-shift systems typically have shorter range, they can achieve higher precision at close distances and are more compact.\n\nTwo-dimensional LiDAR scanners rotate a single laser beam in a horizontal plane, producing a 360-degree profile of the environment at a fixed height. These sensors excel at indoor navigation and obstacle detection for ground robots. Three-dimensional LiDAR sensors add vertical scanning to create point clouds—collections of (x, y, z) coordinates representing surfaces in three-dimensional space.",
      "content_type": "mixed",
      "difficulty": "intermediate",
      "keywords": ["LiDAR", "time-of-flight", "distance measurement", "point cloud", "pulsed LiDAR", "phase-shift LiDAR", "2D LiDAR", "3D LiDAR"],
      "char_count": 796,
      "token_estimate": 199
    },
    {
      "chunk_id": "ch3-sec2-pcl-processing",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "LiDAR Sensors",
      "section_number": 2,
      "subsection": "Point Cloud Processing with the Point Cloud Library",
      "subsection_number": 3,
      "heading_level": 3,
      "parent_heading": "3.2 LiDAR Sensors",
      "chunk_order": 6,
      "content": "Raw LiDAR data arrives as streams of distance measurements with associated angles. Converting these into useful representations for navigation and perception requires several processing steps. The Point Cloud Library (PCL) provides standardized algorithms for this purpose.\n\n```python\nimport pcl\nimport numpy as np\n\n# Load a point cloud from ROS 2 bag or PCD file\ncloud = pcl.load_XYZI('lidar_scan.pcd')\n\n# Step 1: Statistical outlier removal\n# Remove points that are isolated (likely noise)\noutlier_filter = cloud.make_statistical_outlier_filter()\noutlier_filter.set_mean_k(50)\noutlier_filter.set_std_dev_mul_thresh(1.0)\ncloud_filtered = outlier_filter.filter()\n\n# Step 2: Voxel grid downsampling\n# Create a 5cm voxel grid, keeping one point per voxel\nvox = cloud_filtered.make_voxel_grid_filter()\nvox.set_leaf_size(0.05, 0.05, 0.05)\ncloud_downsampled = vox.filter()\n\n# Step 3: RANSAC plane segmentation\n# Detect the dominant ground plane\nseg = cloud_downsampled.make_segmenter()\nseg.set_model_type(pcl.SACMODEL_PLANE)\nseg.set_method_type(pcl.SAC_RANSAC)\nseg.set_distance_threshold(0.03)\ninliers, coefficients = seg.segment()\n\n# Extract ground points and obstacles\nground_cloud = cloud_downsampled.extract(inliers, negative=False)\nobstacle_cloud = cloud_downsampled.extract(inliers, negative=True)\n```\n\nThis pipeline filters noise, reduces data density for efficient processing, and separates ground plane from obstacles using RANSAC plane segmentation.",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["Point Cloud Library", "PCL", "outlier removal", "voxel grid", "RANSAC", "plane segmentation", "point cloud processing", "obstacle detection"],
      "char_count": 1048,
      "token_estimate": 262
    },
    {
      "chunk_id": "ch3-sec2-ros2-integration",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "LiDAR Sensors",
      "section_number": 2,
      "subsection": "ROS 2 Integration",
      "subsection_number": 4,
      "heading_level": 3,
      "parent_heading": "3.2 LiDAR Sensors",
      "chunk_order": 7,
      "content": "In ROS 2 systems, LiDAR data typically flows through the sensor_msgs/msg/LaserScan message type for 2D scanners or sensor_msgs/msg/PointCloud2 for 3D sensors. The following ROS 2 Python node demonstrates subscribing to the Unitree G1's Mid-360 LiDAR topic and detecting obstacles within a safety zone.\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Float32MultiArray\nimport numpy as np\n\nclass LidarObstacleNode(Node):\n    def __init__(self):\n        super().__init__('lidar_obstacle_detector')\n        self.subscription = self.create_subscription(\n            PointCloud2, '/mid360_ros2/cloud',\n            self.pointcloud_callback, 10)\n        self.safety_publisher = self.create_publisher(\n            Float32MultiArray, '/obstacles/safety_zone', 10)\n        self.safety_threshold = 0.5  # meters\n\n    def pointcloud_callback(self, msg):\n        points = self.parse_pointcloud2(msg)\n        distances = np.linalg.norm(points[:, :3], axis=1)\n        close_points = points[distances < self.safety_threshold]\n        if len(close_points) > 0:\n            obstacle_msg = Float32MultiArray()\n            obstacle_msg.data = [float(len(close_points)), float(np.min(distances))]\n            self.safety_publisher.publish(obstacle_msg)\n\n    def parse_pointcloud2(self, msg):\n        dtype = np.dtype([('x', np.float32), ('y', np.float32),\n                          ('z', np.float32), ('intensity', np.float32)])\n        data = np.frombuffer(msg.data, dtype=dtype)\n        return np.column_stack([data['x'], data['y'], data['z'], data['intensity']])\n```",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["ROS 2", "PointCloud2", "LiDAR obstacle detection", "ROS 2 node", "Unitree Mid-360", "safety zone", "sensor_msgs"],
      "char_count": 1008,
      "token_estimate": 252
    },
    {
      "chunk_id": "ch3-sec3-rgbd-overview",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "RGB-D Cameras",
      "section_number": 3,
      "subsection": "The Fusion of Color and Depth",
      "subsection_number": 1,
      "heading_level": 3,
      "parent_heading": "3.3 RGB-D Cameras",
      "chunk_order": 8,
      "content": "RGB-D cameras combine conventional color imaging with depth sensing, providing both the visual texture of a scene and the geometric structure of surfaces within it. The 'D' stands for depth, and these sensors have transformed robotic manipulation and indoor navigation by making three-dimensional perception accessible at low cost and compact form factors. Where a standard camera produces a 2D array of color values, an RGB-D camera produces a 2D array where each pixel encodes both color and distance from the sensor.\n\nThree distinct technologies enable commercial RGB-D cameras today. Structured-light cameras project a known pattern onto the scene and observe how that pattern deforms on surfaces. The Intel RealSense D400 series employs this approach with a laser projector that casts a pseudorandom infrared dot pattern. Stereo cameras use two conventional color cameras separated by a known baseline distance, computing depth through triangulation. Time-of-Flight cameras measure depth by timing the round-trip of emitted light, similar to LiDAR but typically using modulated illumination.",
      "content_type": "paragraph",
      "difficulty": "intermediate",
      "keywords": ["RGB-D cameras", "depth sensing", "structured light", "stereo vision", "time-of-flight", "RealSense", "depth map"],
      "char_count": 595,
      "token_estimate": 149
    },
    {
      "chunk_id": "ch3-sec3-calibration",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "RGB-D Cameras",
      "section_number": 3,
      "subsection": "Camera Calibration and the Intrinsic Matrix",
      "subsection_number": 4,
      "heading_level": 3,
      "parent_heading": "3.3 RGB-D Cameras",
      "chunk_order": 9,
      "content": "Accurate depth and position measurements from RGB-D cameras require precise calibration of the relationship between image coordinates and physical directions. This relationship is captured in the camera intrinsic matrix, which transforms normalized coordinates to pixel coordinates. The parameters are the focal lengths (fx, fy) in pixels, the optical center (cx, cy) in pixel coordinates, and the depth scaling factor. Radial and tangential distortion coefficients account for lens imperfections.\n\n```python\nimport cv2\nimport numpy as np\nimport glob\n\nobjp = np.zeros((6*7, 3), np.float32)\nobjp[:, :2] = np.mgrid[0:7, 0:6].T.reshape(-1, 2)\nobj_points = []\nimg_points = []\n\nimages = glob.glob('calibration_images/*.png')\nfor fname in images:\n    img = cv2.imread(fname)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ret, corners = cv2.findChessboardCorners(gray, (7, 6), None)\n    if ret:\n        obj_points.append(objp)\n        refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1),\n            criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))\n        img_points.append(refined)\n\nret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n    obj_points, img_points, gray.shape[::-1], None, None)\nprint(\"Camera Matrix:\", camera_matrix)\nprint(\"Distortion Coefficients:\", dist_coeffs)\nnp.save('camera_matrix.npy', camera_matrix)\nnp.save('distortion_coeffs.npy', dist_coeffs)\n```",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["camera calibration", "intrinsic matrix", "OpenCV", "checkerboard", "focal length", "distortion coefficients", "camera_matrix"],
      "char_count": 1066,
      "token_estimate": 267
    },
    {
      "chunk_id": "ch3-sec3-realsense",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "RGB-D Cameras",
      "section_number": 3,
      "subsection": "Unitree G1 RealSense D435i Integration",
      "subsection_number": 5,
      "heading_level": 3,
      "parent_heading": "3.3 RGB-D Cameras",
      "chunk_order": 10,
      "content": "The Unitree G1 incorporates the Intel RealSense D435i, an updated version of the popular D435 with an integrated IMU. This camera provides synchronized RGB and depth imaging at resolutions up to 1280 x 720 pixels for depth and 1920 x 1080 for color. The RGB sensor has an 87-degree diagonal field of view, while the depth module covers 69 degrees horizontally and 42 degrees vertically. Operating range extends from 0.3 to 10 meters, with depth accuracy typically within 2% of the measured distance.\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass RealsenseNode(Node):\n    def __init__(self):\n        super().__init__('realsense_node')\n        self.bridge = CvBridge()\n        self.color_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.color_callback, 10)\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth/image_rect_raw', self.depth_callback, 10)\n        self.info_sub = self.create_subscription(\n            CameraInfo, '/camera/color/camera_info', self.info_callback, 10)\n        self.camera_matrix = None\n        self.latest_depth = None\n\n    def get_point_cloud(self, u, v):\n        if self.camera_matrix is None or self.latest_depth is None:\n            return None\n        Z = self.latest_depth[v, u]\n        if np.isnan(Z) or Z == 0:\n            return None\n        fx, fy = self.camera_matrix[0, 0], self.camera_matrix[1, 1]\n        cx, cy = self.camera_matrix[0, 2], self.camera_matrix[1, 2]\n        X = (u - cx) * Z / fx\n        Y = (v - cy) * Z / fy\n        return np.array([X, Y, Z])\n```",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["RealSense D435i", "RGB-D integration", "ROS 2", "cv_bridge", "depth image", "point cloud conversion", "Unitree G1"],
      "char_count": 1092,
      "token_estimate": 273
    },
    {
      "chunk_id": "ch3-sec4-imu-principles",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Inertial Measurement Units",
      "section_number": 4,
      "subsection": "Measuring Motion Directly",
      "subsection_number": 1,
      "heading_level": 3,
      "parent_heading": "3.4 Inertial Measurement Units",
      "chunk_order": 11,
      "content": "Inertial Measurement Units (IMUs) measure the forces acting on a body, from which its acceleration and angular rotation can be inferred. These sensors provide direct measurements of motion without reference to external landmarks, making them essential for tracking robot orientation and detecting impacts. The Unitree G1 incorporates a Bosch BMI088 IMU that combines a 3-axis accelerometer and 3-axis gyroscope, sampling at 1 kHz to capture rapid motions during locomotion and manipulation.\n\nAn accelerometer measures specific force—the vector sum of gravitational acceleration and the kinematic acceleration of the sensor body. In the sensor frame, the reading a_raw relates to the true acceleration a_true and gravitational acceleration g as: a_raw = R * a_true + g + b_a + n_a, where R is the rotation matrix from world to sensor frame, b_a is the accelerometer bias, and n_a is measurement noise. When stationary, the accelerometer reads only gravity, enabling orientation estimation.\n\nA gyroscope measures angular velocity omega directly, providing the rate of change of orientation. Integration of angular velocity gives orientation, but bias errors accumulate as constant offset in the integrated angle. A gyroscope bias of just 0.1 deg/s accumulates to a 36-degree orientation error after 6 minutes—a significant problem for systems relying on gyroscope integration alone.\n\nIMU measurements are corrupted by white noise (rapid random fluctuations), bias instability (low-frequency bias wander), and temperature sensitivity. The noise density is specified in mg/√Hz for accelerometers and deg/s/√Hz for gyroscopes.",
      "content_type": "mixed",
      "difficulty": "advanced",
      "keywords": ["IMU", "accelerometer", "gyroscope", "angular velocity", "bias instability", "noise density", "BMI088", "orientation estimation"],
      "char_count": 958,
      "token_estimate": 240
    },
    {
      "chunk_id": "ch3-sec4-complementary-filter",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Inertial Measurement Units",
      "section_number": 4,
      "subsection": "Complementary Filtering for Sensor Fusion",
      "subsection_number": 4,
      "heading_level": 3,
      "parent_heading": "3.4 Inertial Measurement Units",
      "chunk_order": 12,
      "content": "The accelerometer and gyroscope provide complementary information about orientation. The gyroscope accurately tracks rapid orientation changes but drifts over time. The accelerometer provides absolute orientation reference (through gravity direction) but is corrupted by linear accelerations during motion. A complementary filter combines these sources by applying high-pass filtering to gyroscope data and low-pass filtering to accelerometer data.\n\n```python\nimport numpy as np\n\nclass ComplementaryFilter:\n    def __init__(self, alpha=0.98, sample_time=0.001):\n        self.alpha = alpha\n        self.dt = sample_time\n        self.roll = 0.0\n        self.pitch = 0.0\n\n    def update(self, ax, ay, az, gx, gy, gz):\n        # Convert gyroscope readings from deg/s to rad/s\n        gx_rad = np.deg2rad(gx)\n        gy_rad = np.deg2rad(gy)\n\n        # Integrate gyroscope for orientation change\n        self.roll += gx_rad * self.dt\n        self.pitch += gy_rad * self.dt\n\n        # Compute orientation from accelerometer\n        accel_roll = np.arctan2(ay, np.sqrt(ax**2 + az**2))\n        accel_pitch = np.arctan2(-ax, np.sqrt(ay**2 + az**2))\n\n        # Complementary filter: blend gyro integration with accel reference\n        self.roll = self.alpha * self.roll + (1 - self.alpha) * accel_roll\n        self.pitch = self.alpha * self.pitch + (1 - self.alpha) * accel_pitch\n        return self.roll, self.pitch\n```\n\nThe alpha parameter controls the filter cutoff—higher alpha trusts the gyroscope more and attenuates accelerometer noise more strongly.",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["complementary filter", "sensor fusion", "orientation estimation", "high-pass filter", "low-pass filter", "IMU filtering"],
      "char_count": 1042,
      "token_estimate": 261
    },
    {
      "chunk_id": "ch3-sec4-madgwick-filter",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Inertial Measurement Units",
      "section_number": 4,
      "subsection": "Advanced Sensor Fusion: Madgwick and Mahony Filters",
      "subsection_number": 5,
      "heading_level": 3,
      "parent_heading": "3.4 Inertial Measurement Units",
      "chunk_order": 13,
      "content": "For applications requiring full 3D orientation estimation with higher accuracy, the Madgwick and Mahony filters provide sophisticated solutions. These filters use quaternion representation to avoid gimbal lock and incorporate magnetometer data for heading (yaw) reference. The Madgwick filter update equation is: q_dot = 0.5 * q * omega + beta * (gradient descent term), where q is the quaternion representing orientation, omega is the angular velocity quaternion, and beta is a filter gain parameter controlling correction aggressiveness.\n\n```python\nimport numpy as np\n\nclass MadgwickFilter:\n    def __init__(self, beta=0.1, sample_period=0.001):\n        self.beta = beta\n        self.q = np.array([1.0, 0.0, 0.0, 0.0])  # Initial quaternion\n        self.sample_period = sample_period\n\n    def update(self, accel, gyro, mag=None):\n        q = self.q.copy()\n        ax, ay, az = accel\n        gx, gy, gz = gyro\n        norm = np.sqrt(ax**2 + ay**2 + az**2)\n        ax, ay, az = ax/norm, ay/norm, az/norm\n\n        # Gradient descent step for orientation correction\n        f = np.array([2*(q1*q3 - q0*q2) - ax,\n                      2*(q0*q1 + q2*q3) - ay,\n                      2*(0.5 - q1**2 - q2**2) - az])\n        J = np.array([[-2*q2, 2*q3, -2*q0, 2*q1],\n                      [2*q1, 2*q0, 2*q3, 2*q2],\n                      [0, -4*q1, -4*q2, 0]])\n        step = J.T @ f / np.linalg.norm(J.T @ f)\n\n        # Gyroscope quaternion rate of change\n        q_dot = 0.5 * np.array([\n            -q1*gx - q2*gy - q3*gz,\n            q0*gx + q2*gz - q3*gy,\n            q0*gy - q1*gz + q3*gx,\n            q0*gz + q1*gy - q2*gx\n        ])\n        self.q = (q + (q_dot - self.beta * step) * self.sample_period)\n        self.q = self.q / np.linalg.norm(self.q)\n        return self.q\n```",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["Madgwick filter", "quaternion", "sensor fusion", "3D orientation", "gimbal lock", "gradient descent", "AHRS"],
      "char_count": 1232,
      "token_estimate": 308
    },
    {
      "chunk_id": "ch3-sec5-ft-sensors",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Force-Torque Sensors",
      "section_number": 5,
      "subsection": null,
      "subsection_number": null,
      "heading_level": 2,
      "parent_heading": "3.5 Force-Torque Sensors",
      "chunk_order": 14,
      "content": "Force-torque (F/T) sensors measure the full six-dimensional wrench—three forces and three torques—applied at a sensing interface. These sensors enable robots to perform delicate manipulation tasks where precise control of contact forces is essential. Most F/T sensors operate on the principle of elastic deformation under load. When a force or torque is applied, internal structures deform slightly—typically micrometers. Precision strain gauges bonded to these structures convert deformation into electrical resistance changes, measured using Wheatstone bridge circuits.\n\n```python\nclass ForceTorqueSensor:\n    def __init__(self, calibration_matrix, load_mass, load_com):\n        self.calibration = calibration_matrix\n        self.load_mass = load_mass\n        self.load_com = load_com\n        self.g = np.array([0, 0, -9.81])\n        self.zero_offset = np.zeros(6)\n\n    def set_zero(self, raw_readings):\n        self.zero_offset = self.calibration @ raw_readings\n\n    def read_wrench(self, raw_readings):\n        wrench = self.calibration @ raw_readings - self.zero_offset\n        wrench[:3] -= self.load_mass * self.g\n        com = np.array(self.load_com)\n        wrench[3:] -= np.cross(com, self.load_mass * self.g)\n        return wrench\n```\n\nF/T sensors enable impedance control for safe human-robot collaboration, precision assembly operations, and manipulation of deformable objects. They are sensitive to temperature changes and have limited overload ratings.",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["force-torque sensor", "strain gauge", "Wheatstone bridge", "wrench", "impedance control", "gravity compensation", "6-axis sensor"],
      "char_count": 988,
      "token_estimate": 247
    },
    {
      "chunk_id": "ch3-sec6-actuator-fundamentals",
      "chapter": "Sensors and Actuators",
      "chapter_number": 3,
      "section": "Actuators",
      "section_number": 6,
      "subsection": null,
      "subsection_number": null,
      "heading_level": 2,
      "parent_heading": "3.6 Actuators",
      "chunk_order": 15,
      "content": "Actuators are the muscles of robotic systems, converting electrical, hydraulic, or pneumatic energy into controlled motion. The workhorse of modern robotics is the brushed or brushless DC motor. When current flows through a motor's windings in the presence of a magnetic field, Lorentz forces produce rotation. The back EMF voltage E generated by a spinning motor is proportional to its angular velocity: E = k_e * omega. The torque constant k_t relates current to developed torque: tau = k_t * I. For SI units, k_e equals k_t.\n\n```python\nclass DCMotor:\n    def __init__(self, R=1.0, L=0.001, k_e=0.01, k_t=0.01, J=0.001, B=0.001):\n        self.R = R      # Terminal resistance (ohms)\n        self.L = L      # Terminal inductance (henries)\n        self.k_e = k_e  # Back EMF constant (V·s/rad)\n        self.k_t = k_t  # Torque constant (N·m/A)\n        self.J = J      # Rotor inertia (kg·m²)\n        self.B = B      # Viscous damping (N·m·s/rad)\n\n    def torque_speed_curve(self, voltage, load_torque=0):\n        currents = np.linspace(0, voltage / self.R, 100)\n        torques = self.k_t * currents\n        speeds = (voltage - self.R * currents) / self.k_e\n        return torques, speeds\n```\n\nServo motors integrate a DC motor with position feedback and control electronics. Stepper motors divide rotation into discrete steps, enabling open-loop position control. The Unitree G1 employs direct-drive actuators with brushless motors without reduction gearing, providing 70 Nm peak torque per joint for precise torque control. Motor datasheets quote peak torques and no-load speeds that cannot be achieved simultaneously—plot the required torque-speed curve for your task and verify it lies within the continuous operating region.",
      "content_type": "code",
      "difficulty": "advanced",
      "keywords": ["DC motor", "actuator", "back EMF", "torque constant", "servo motor", "stepper motor", "direct-drive", "torque-speed curve", "Unitree G1"],
      "char_count": 1232,
      "token_estimate": 308
    }
  ]
}

# Chapter 12 Labs: Vision-Language-Action Models

This directory contains lab exercises for Chapter 12: Vision-Language-Action Models.

## Lab Overview

The labs in this chapter focus on practical implementation of Vision-Language-Action Models concepts using the Unitree G1 robot and ROS 2 Humble.

## Labs Included

1. **Lab 12.1**: Introduction to Vision-Language-Action Models - Basic Concepts
2. **Lab 12.2**: Intermediate Vision-Language-Action Models - Implementation
3. **Lab 12.3**: Advanced Vision-Language-Action Models - Integration and Optimization

## Prerequisites

- Docker and docker-compose
- Basic ROS 2 knowledge
- Understanding of Vision-Language-Action Models theory (Chapter 12)

## Getting Started

Each lab has its own directory with:
- Dockerfile for the lab environment
- ROS 2 nodes implementing Vision-Language-Action Models concepts
- Launch files for easy execution
- Documentation and assessment criteria

To run a specific lab:
1. Navigate to the lab directory
2. Follow the instructions in the README.md file
3. Use the provided Makefile for common operations

## Assessment

Each lab includes assessment criteria to validate your implementation of Vision-Language-Action Models concepts.

---

**Lab directory created**: 2026-01-05T23:06:04.378997
**Chapter**: 12 - Vision-Language-Action Models
